\section{Introduction}

In recent years, deep neural networks have been proposed to improve the effectiveness of neural networks in various machine learning tasks such as image classification, natural language processing, speech recognition, etc. However, the models learned by deep learning systems tend to be very complex and are not easily understandable by humans. This can be problematic when deep neural networks are used to make decisions that have a societal impact such as hiring, credit decisions, prison sentencing, etc. In such applications, it is necessary to check that the decision was made in a fair manner. For example, if an algorithm decides that a job offer should not be made then this decision should have been based on the basis of the candidate's lack of required skill and not on the basis of their gender or ethnicity.

The goal of our project is to generate explanations for predictions made by a deep neural network.